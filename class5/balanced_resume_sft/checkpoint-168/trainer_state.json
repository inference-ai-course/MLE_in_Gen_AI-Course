{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 168,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 7.947960376739502,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 2.8809,
      "step": 2
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.787482500076294,
      "learning_rate": 8.999999999999999e-05,
      "loss": 2.6695,
      "step": 4
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 5.191319942474365,
      "learning_rate": 0.00015,
      "loss": 2.702,
      "step": 6
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 2.3441073894500732,
      "learning_rate": 0.00020999999999999998,
      "loss": 2.2914,
      "step": 8
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 2.3990180492401123,
      "learning_rate": 0.00027,
      "loss": 2.2194,
      "step": 10
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 2.39736270904541,
      "learning_rate": 0.0002999703494655098,
      "loss": 1.8843,
      "step": 12
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.0150184631347656,
      "learning_rate": 0.0002997332155172899,
      "loss": 1.8873,
      "step": 14
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 3.0965497493743896,
      "learning_rate": 0.00029925932257836405,
      "loss": 1.563,
      "step": 16
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 4.027049541473389,
      "learning_rate": 0.00029854941997087525,
      "loss": 1.521,
      "step": 18
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 3.3812520503997803,
      "learning_rate": 0.0002976046301967631,
      "loss": 1.5576,
      "step": 20
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 2.5942225456237793,
      "learning_rate": 0.0002964264471628576,
      "loss": 1.4414,
      "step": 22
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 3.012852907180786,
      "learning_rate": 0.00029501673381870556,
      "loss": 1.2204,
      "step": 24
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 3.1220977306365967,
      "learning_rate": 0.0002933777192108641,
      "loss": 1.394,
      "step": 26
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.500534772872925,
      "learning_rate": 0.00029151199495832015,
      "loss": 0.8601,
      "step": 28
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 3.760908842086792,
      "learning_rate": 0.000289422511154608,
      "loss": 1.1169,
      "step": 30
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 2.9442670345306396,
      "learning_rate": 0.0002871125717031052,
      "loss": 1.1759,
      "step": 32
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 2.0454702377319336,
      "learning_rate": 0.00028458582909288177,
      "loss": 0.5897,
      "step": 34
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 3.6448488235473633,
      "learning_rate": 0.00028184627862336503,
      "loss": 0.8536,
      "step": 36
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 3.416459560394287,
      "learning_rate": 0.00027889825208694916,
      "loss": 0.8078,
      "step": 38
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 3.2605955600738525,
      "learning_rate": 0.00027574641091954145,
      "loss": 0.685,
      "step": 40
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.9928762912750244,
      "learning_rate": 0.0002723957388298741,
      "loss": 0.4666,
      "step": 42
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 2.38954496383667,
      "learning_rate": 0.0002688515339192361,
      "loss": 0.5085,
      "step": 44
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 2.113816976547241,
      "learning_rate": 0.0002651194003040862,
      "loss": 0.3974,
      "step": 46
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 10.709357261657715,
      "learning_rate": 0.0002612052392547927,
      "loss": 0.4726,
      "step": 48
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 2.072464942932129,
      "learning_rate": 0.0002571152398645124,
      "loss": 0.2912,
      "step": 50
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 2.4414174556732178,
      "learning_rate": 0.000252855869262962,
      "loss": 0.3353,
      "step": 52
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 6.433282852172852,
      "learning_rate": 0.0002484338623905563,
      "loss": 0.467,
      "step": 54
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 3.153963804244995,
      "learning_rate": 0.00024385621134908396,
      "loss": 0.2752,
      "step": 56
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 2.254190683364868,
      "learning_rate": 0.00023913015434575658,
      "loss": 0.2802,
      "step": 58
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 3.8183329105377197,
      "learning_rate": 0.0002342631642481156,
      "loss": 0.3657,
      "step": 60
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 1.0533329248428345,
      "learning_rate": 0.00022926293676789293,
      "loss": 0.2885,
      "step": 62
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 1.0564547777175903,
      "learning_rate": 0.00022413737829250841,
      "loss": 0.2218,
      "step": 64
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 2.784341812133789,
      "learning_rate": 0.00021889459338344665,
      "loss": 0.2395,
      "step": 66
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 2.773198127746582,
      "learning_rate": 0.00021354287196127925,
      "loss": 0.2107,
      "step": 68
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 2.758924722671509,
      "learning_rate": 0.00020809067619759618,
      "loss": 0.2654,
      "step": 70
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 1.1660783290863037,
      "learning_rate": 0.00020254662713457364,
      "loss": 0.2257,
      "step": 72
    },
    {
      "epoch": 3.5238095238095237,
      "grad_norm": 3.140939474105835,
      "learning_rate": 0.00019691949105333408,
      "loss": 0.241,
      "step": 74
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 4.222166538238525,
      "learning_rate": 0.00019121816561265374,
      "loss": 0.2838,
      "step": 76
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 2.276317834854126,
      "learning_rate": 0.00018545166577993574,
      "loss": 0.2315,
      "step": 78
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 1.3264436721801758,
      "learning_rate": 0.0001796291095766929,
      "loss": 0.1917,
      "step": 80
    },
    {
      "epoch": 3.9047619047619047,
      "grad_norm": 1.3361318111419678,
      "learning_rate": 0.00017375970366108223,
      "loss": 0.2773,
      "step": 82
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.5183502435684204,
      "learning_rate": 0.00016785272877028573,
      "loss": 0.1794,
      "step": 84
    },
    {
      "epoch": 4.095238095238095,
      "grad_norm": 0.42316362261772156,
      "learning_rate": 0.00016191752504575714,
      "loss": 0.1998,
      "step": 86
    },
    {
      "epoch": 4.190476190476191,
      "grad_norm": 0.570604681968689,
      "learning_rate": 0.00015596347726453888,
      "loss": 0.2779,
      "step": 88
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 1.426870346069336,
      "learning_rate": 0.00015,
      "loss": 0.1557,
      "step": 90
    },
    {
      "epoch": 4.380952380952381,
      "grad_norm": 0.6735051274299622,
      "learning_rate": 0.00014403652273546117,
      "loss": 0.2402,
      "step": 92
    },
    {
      "epoch": 4.476190476190476,
      "grad_norm": 2.725604295730591,
      "learning_rate": 0.00013808247495424283,
      "loss": 0.2779,
      "step": 94
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.7067520618438721,
      "learning_rate": 0.0001321472712297143,
      "loss": 0.1672,
      "step": 96
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 1.3722397089004517,
      "learning_rate": 0.00012624029633891774,
      "loss": 0.1343,
      "step": 98
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 3.063159704208374,
      "learning_rate": 0.0001203708904233071,
      "loss": 0.1613,
      "step": 100
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 3.5889596939086914,
      "learning_rate": 0.00011454833422006428,
      "loss": 0.1454,
      "step": 102
    },
    {
      "epoch": 4.9523809523809526,
      "grad_norm": 0.8698517084121704,
      "learning_rate": 0.00010878183438734619,
      "loss": 0.2347,
      "step": 104
    },
    {
      "epoch": 5.0476190476190474,
      "grad_norm": 0.322376012802124,
      "learning_rate": 0.00010308050894666593,
      "loss": 0.1334,
      "step": 106
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 0.5109057426452637,
      "learning_rate": 9.745337286542634e-05,
      "loss": 0.1507,
      "step": 108
    },
    {
      "epoch": 5.238095238095238,
      "grad_norm": 0.7742757201194763,
      "learning_rate": 9.190932380240384e-05,
      "loss": 0.1509,
      "step": 110
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.6492786407470703,
      "learning_rate": 8.645712803872084e-05,
      "loss": 0.1221,
      "step": 112
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 0.41702958941459656,
      "learning_rate": 8.110540661655328e-05,
      "loss": 0.2153,
      "step": 114
    },
    {
      "epoch": 5.523809523809524,
      "grad_norm": 1.676308035850525,
      "learning_rate": 7.586262170749157e-05,
      "loss": 0.1416,
      "step": 116
    },
    {
      "epoch": 5.619047619047619,
      "grad_norm": 0.3152313530445099,
      "learning_rate": 7.073706323210713e-05,
      "loss": 0.1643,
      "step": 118
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 0.29023751616477966,
      "learning_rate": 6.573683575188433e-05,
      "loss": 0.2099,
      "step": 120
    },
    {
      "epoch": 5.809523809523809,
      "grad_norm": 0.39325541257858276,
      "learning_rate": 6.0869845654243425e-05,
      "loss": 0.2209,
      "step": 122
    },
    {
      "epoch": 5.904761904761905,
      "grad_norm": 0.5661391615867615,
      "learning_rate": 5.6143788650916e-05,
      "loss": 0.1434,
      "step": 124
    },
    {
      "epoch": 6.0,
      "grad_norm": 4.754378795623779,
      "learning_rate": 5.156613760944369e-05,
      "loss": 0.1492,
      "step": 126
    },
    {
      "epoch": 6.095238095238095,
      "grad_norm": 0.32981154322624207,
      "learning_rate": 4.714413073703804e-05,
      "loss": 0.1237,
      "step": 128
    },
    {
      "epoch": 6.190476190476191,
      "grad_norm": 0.28579309582710266,
      "learning_rate": 4.288476013548755e-05,
      "loss": 0.1361,
      "step": 130
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 0.39466017484664917,
      "learning_rate": 3.879476074520731e-05,
      "loss": 0.1487,
      "step": 132
    },
    {
      "epoch": 6.380952380952381,
      "grad_norm": 0.23920992016792297,
      "learning_rate": 3.488059969591378e-05,
      "loss": 0.123,
      "step": 134
    },
    {
      "epoch": 6.476190476190476,
      "grad_norm": 0.46962499618530273,
      "learning_rate": 3.114846608076387e-05,
      "loss": 0.1465,
      "step": 136
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 0.28070536255836487,
      "learning_rate": 2.7604261170125892e-05,
      "loss": 0.1169,
      "step": 138
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.6189239025115967,
      "learning_rate": 2.425358908045851e-05,
      "loss": 0.2134,
      "step": 140
    },
    {
      "epoch": 6.761904761904762,
      "grad_norm": 0.40509626269340515,
      "learning_rate": 2.1101747913050855e-05,
      "loss": 0.1112,
      "step": 142
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 0.5502594113349915,
      "learning_rate": 1.8153721376634944e-05,
      "loss": 0.1641,
      "step": 144
    },
    {
      "epoch": 6.9523809523809526,
      "grad_norm": 0.5221810936927795,
      "learning_rate": 1.541417090711815e-05,
      "loss": 0.1355,
      "step": 146
    },
    {
      "epoch": 7.0476190476190474,
      "grad_norm": 0.4170115292072296,
      "learning_rate": 1.2887428296894809e-05,
      "loss": 0.1276,
      "step": 148
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 0.323545902967453,
      "learning_rate": 1.0577488845391947e-05,
      "loss": 0.2126,
      "step": 150
    },
    {
      "epoch": 7.238095238095238,
      "grad_norm": 0.5822673439979553,
      "learning_rate": 8.488005041679841e-06,
      "loss": 0.2022,
      "step": 152
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.33242395520210266,
      "learning_rate": 6.622280789135876e-06,
      "loss": 0.1195,
      "step": 154
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 0.35205212235450745,
      "learning_rate": 4.983266181294443e-06,
      "loss": 0.1049,
      "step": 156
    },
    {
      "epoch": 7.523809523809524,
      "grad_norm": 0.3080903887748718,
      "learning_rate": 3.5735528371423904e-06,
      "loss": 0.1791,
      "step": 158
    },
    {
      "epoch": 7.619047619047619,
      "grad_norm": 0.44864654541015625,
      "learning_rate": 2.395369803236902e-06,
      "loss": 0.1376,
      "step": 160
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 0.45603135228157043,
      "learning_rate": 1.4505800291247205e-06,
      "loss": 0.1208,
      "step": 162
    },
    {
      "epoch": 7.809523809523809,
      "grad_norm": 0.3028973340988159,
      "learning_rate": 7.40677421635888e-07,
      "loss": 0.1316,
      "step": 164
    },
    {
      "epoch": 7.904761904761905,
      "grad_norm": 0.3432357609272003,
      "learning_rate": 2.667844827100529e-07,
      "loss": 0.1178,
      "step": 166
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.6184660792350769,
      "learning_rate": 2.965053449016652e-08,
      "loss": 0.2115,
      "step": 168
    }
  ],
  "logging_steps": 2,
  "max_steps": 168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 20,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 758631591051264.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
