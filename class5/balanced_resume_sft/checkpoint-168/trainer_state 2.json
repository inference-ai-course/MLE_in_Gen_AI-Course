{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 168,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 7.160931587219238,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 2.8001,
      "step": 2
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 2.91804575920105,
      "learning_rate": 8.999999999999999e-05,
      "loss": 2.5727,
      "step": 4
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 5.078065395355225,
      "learning_rate": 0.00015,
      "loss": 2.6927,
      "step": 6
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 2.4128193855285645,
      "learning_rate": 0.00020999999999999998,
      "loss": 2.2391,
      "step": 8
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 2.54167103767395,
      "learning_rate": 0.00027,
      "loss": 2.3408,
      "step": 10
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 2.4027938842773438,
      "learning_rate": 0.0002999703494655098,
      "loss": 1.9093,
      "step": 12
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.0930118560791016,
      "learning_rate": 0.0002997332155172899,
      "loss": 1.939,
      "step": 14
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 3.0570709705352783,
      "learning_rate": 0.00029925932257836405,
      "loss": 1.5286,
      "step": 16
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 3.667117118835449,
      "learning_rate": 0.00029854941997087525,
      "loss": 1.5628,
      "step": 18
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 3.7415225505828857,
      "learning_rate": 0.0002976046301967631,
      "loss": 1.4784,
      "step": 20
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 2.7100160121917725,
      "learning_rate": 0.0002964264471628576,
      "loss": 1.4286,
      "step": 22
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 2.6789116859436035,
      "learning_rate": 0.00029501673381870556,
      "loss": 0.8397,
      "step": 24
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 2.755162239074707,
      "learning_rate": 0.0002933777192108641,
      "loss": 1.1052,
      "step": 26
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.3215315341949463,
      "learning_rate": 0.00029151199495832015,
      "loss": 0.8591,
      "step": 28
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 3.5819156169891357,
      "learning_rate": 0.000289422511154608,
      "loss": 0.8933,
      "step": 30
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 3.382352828979492,
      "learning_rate": 0.0002871125717031052,
      "loss": 1.4071,
      "step": 32
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 1.3760684728622437,
      "learning_rate": 0.00028458582909288177,
      "loss": 0.4732,
      "step": 34
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 3.5447652339935303,
      "learning_rate": 0.00028184627862336503,
      "loss": 0.8164,
      "step": 36
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 3.466907024383545,
      "learning_rate": 0.00027889825208694916,
      "loss": 0.7967,
      "step": 38
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 3.0145747661590576,
      "learning_rate": 0.00027574641091954145,
      "loss": 0.6775,
      "step": 40
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.1624622344970703,
      "learning_rate": 0.0002723957388298741,
      "loss": 0.5578,
      "step": 42
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 3.1159603595733643,
      "learning_rate": 0.0002688515339192361,
      "loss": 0.4703,
      "step": 44
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 2.591205596923828,
      "learning_rate": 0.0002651194003040862,
      "loss": 0.3955,
      "step": 46
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 2.5336501598358154,
      "learning_rate": 0.0002612052392547927,
      "loss": 0.382,
      "step": 48
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 11.831185340881348,
      "learning_rate": 0.0002571152398645124,
      "loss": 0.2252,
      "step": 50
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 6.068868160247803,
      "learning_rate": 0.000252855869262962,
      "loss": 0.4519,
      "step": 52
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 4.395319938659668,
      "learning_rate": 0.0002484338623905563,
      "loss": 0.3676,
      "step": 54
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.9511734247207642,
      "learning_rate": 0.00024385621134908396,
      "loss": 0.289,
      "step": 56
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 2.291565418243408,
      "learning_rate": 0.00023913015434575658,
      "loss": 0.2395,
      "step": 58
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 3.142674684524536,
      "learning_rate": 0.0002342631642481156,
      "loss": 0.3276,
      "step": 60
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.7584366798400879,
      "learning_rate": 0.00022926293676789293,
      "loss": 0.2049,
      "step": 62
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 1.0362313985824585,
      "learning_rate": 0.00022413737829250841,
      "loss": 0.1981,
      "step": 64
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.9294747710227966,
      "learning_rate": 0.00021889459338344665,
      "loss": 0.1623,
      "step": 66
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 1.2659662961959839,
      "learning_rate": 0.00021354287196127925,
      "loss": 0.1604,
      "step": 68
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 1.1925544738769531,
      "learning_rate": 0.00020809067619759618,
      "loss": 0.1867,
      "step": 70
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 1.2505751848220825,
      "learning_rate": 0.00020254662713457364,
      "loss": 0.1198,
      "step": 72
    },
    {
      "epoch": 3.5238095238095237,
      "grad_norm": 3.8196020126342773,
      "learning_rate": 0.00019691949105333408,
      "loss": 0.18,
      "step": 74
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 9.06416130065918,
      "learning_rate": 0.00019121816561265374,
      "loss": 0.2462,
      "step": 76
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 3.404033899307251,
      "learning_rate": 0.00018545166577993574,
      "loss": 0.2211,
      "step": 78
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 1.8362782001495361,
      "learning_rate": 0.0001796291095766929,
      "loss": 0.1872,
      "step": 80
    },
    {
      "epoch": 3.9047619047619047,
      "grad_norm": 2.2185511589050293,
      "learning_rate": 0.00017375970366108223,
      "loss": 0.2338,
      "step": 82
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.2901296615600586,
      "learning_rate": 0.00016785272877028573,
      "loss": 0.122,
      "step": 84
    },
    {
      "epoch": 4.095238095238095,
      "grad_norm": 0.4141446053981781,
      "learning_rate": 0.00016191752504575714,
      "loss": 0.2175,
      "step": 86
    },
    {
      "epoch": 4.190476190476191,
      "grad_norm": 0.5922152400016785,
      "learning_rate": 0.00015596347726453888,
      "loss": 0.1962,
      "step": 88
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.7426590323448181,
      "learning_rate": 0.00015,
      "loss": 0.0901,
      "step": 90
    },
    {
      "epoch": 4.380952380952381,
      "grad_norm": 0.5514129400253296,
      "learning_rate": 0.00014403652273546117,
      "loss": 0.1768,
      "step": 92
    },
    {
      "epoch": 4.476190476190476,
      "grad_norm": 2.729809522628784,
      "learning_rate": 0.00013808247495424283,
      "loss": 0.2087,
      "step": 94
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 1.1301383972167969,
      "learning_rate": 0.0001321472712297143,
      "loss": 0.0983,
      "step": 96
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.9695544838905334,
      "learning_rate": 0.00012624029633891774,
      "loss": 0.1097,
      "step": 98
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 0.2788452208042145,
      "learning_rate": 0.0001203708904233071,
      "loss": 0.1366,
      "step": 100
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.6646052598953247,
      "learning_rate": 0.00011454833422006428,
      "loss": 0.0945,
      "step": 102
    },
    {
      "epoch": 4.9523809523809526,
      "grad_norm": 0.6823979020118713,
      "learning_rate": 0.00010878183438734619,
      "loss": 0.112,
      "step": 104
    },
    {
      "epoch": 5.0476190476190474,
      "grad_norm": 0.6575085520744324,
      "learning_rate": 0.00010308050894666593,
      "loss": 0.0805,
      "step": 106
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 0.3413795828819275,
      "learning_rate": 9.745337286542634e-05,
      "loss": 0.0947,
      "step": 108
    },
    {
      "epoch": 5.238095238095238,
      "grad_norm": 3.2861568927764893,
      "learning_rate": 9.190932380240384e-05,
      "loss": 0.1123,
      "step": 110
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.5223572254180908,
      "learning_rate": 8.645712803872084e-05,
      "loss": 0.0821,
      "step": 112
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 0.4389367401599884,
      "learning_rate": 8.110540661655328e-05,
      "loss": 0.1767,
      "step": 114
    },
    {
      "epoch": 5.523809523809524,
      "grad_norm": 0.4413521885871887,
      "learning_rate": 7.586262170749157e-05,
      "loss": 0.0744,
      "step": 116
    },
    {
      "epoch": 5.619047619047619,
      "grad_norm": 0.32938194274902344,
      "learning_rate": 7.073706323210713e-05,
      "loss": 0.1446,
      "step": 118
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 0.26184147596359253,
      "learning_rate": 6.573683575188433e-05,
      "loss": 0.0994,
      "step": 120
    },
    {
      "epoch": 5.809523809523809,
      "grad_norm": 0.46222859621047974,
      "learning_rate": 6.0869845654243425e-05,
      "loss": 0.2286,
      "step": 122
    },
    {
      "epoch": 5.904761904761905,
      "grad_norm": 0.3417544662952423,
      "learning_rate": 5.6143788650916e-05,
      "loss": 0.0755,
      "step": 124
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.616649866104126,
      "learning_rate": 5.156613760944369e-05,
      "loss": 0.0948,
      "step": 126
    },
    {
      "epoch": 6.095238095238095,
      "grad_norm": 0.25833022594451904,
      "learning_rate": 4.714413073703804e-05,
      "loss": 0.1006,
      "step": 128
    },
    {
      "epoch": 6.190476190476191,
      "grad_norm": 0.30380505323410034,
      "learning_rate": 4.288476013548755e-05,
      "loss": 0.099,
      "step": 130
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 0.42968738079071045,
      "learning_rate": 3.879476074520731e-05,
      "loss": 0.1137,
      "step": 132
    },
    {
      "epoch": 6.380952380952381,
      "grad_norm": 0.3221842348575592,
      "learning_rate": 3.488059969591378e-05,
      "loss": 0.0866,
      "step": 134
    },
    {
      "epoch": 6.476190476190476,
      "grad_norm": 0.47237005829811096,
      "learning_rate": 3.114846608076387e-05,
      "loss": 0.0576,
      "step": 136
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 0.27665436267852783,
      "learning_rate": 2.7604261170125892e-05,
      "loss": 0.1132,
      "step": 138
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.37555456161499023,
      "learning_rate": 2.425358908045851e-05,
      "loss": 0.1413,
      "step": 140
    },
    {
      "epoch": 6.761904761904762,
      "grad_norm": 0.26877614855766296,
      "learning_rate": 2.1101747913050855e-05,
      "loss": 0.0779,
      "step": 142
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 0.4283902049064636,
      "learning_rate": 1.8153721376634944e-05,
      "loss": 0.0899,
      "step": 144
    },
    {
      "epoch": 6.9523809523809526,
      "grad_norm": 0.4109881520271301,
      "learning_rate": 1.541417090711815e-05,
      "loss": 0.0821,
      "step": 146
    },
    {
      "epoch": 7.0476190476190474,
      "grad_norm": 0.19773121178150177,
      "learning_rate": 1.2887428296894809e-05,
      "loss": 0.1022,
      "step": 148
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 0.3024020195007324,
      "learning_rate": 1.0577488845391947e-05,
      "loss": 0.1913,
      "step": 150
    },
    {
      "epoch": 7.238095238095238,
      "grad_norm": 0.47493258118629456,
      "learning_rate": 8.488005041679841e-06,
      "loss": 0.1352,
      "step": 152
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.30368271470069885,
      "learning_rate": 6.622280789135876e-06,
      "loss": 0.1072,
      "step": 154
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 0.28407788276672363,
      "learning_rate": 4.983266181294443e-06,
      "loss": 0.0854,
      "step": 156
    },
    {
      "epoch": 7.523809523809524,
      "grad_norm": 0.2545681893825531,
      "learning_rate": 3.5735528371423904e-06,
      "loss": 0.0902,
      "step": 158
    },
    {
      "epoch": 7.619047619047619,
      "grad_norm": 0.34992098808288574,
      "learning_rate": 2.395369803236902e-06,
      "loss": 0.0937,
      "step": 160
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 0.4744027554988861,
      "learning_rate": 1.4505800291247205e-06,
      "loss": 0.0482,
      "step": 162
    },
    {
      "epoch": 7.809523809523809,
      "grad_norm": 0.303168922662735,
      "learning_rate": 7.40677421635888e-07,
      "loss": 0.0705,
      "step": 164
    },
    {
      "epoch": 7.904761904761905,
      "grad_norm": 0.3170900344848633,
      "learning_rate": 2.667844827100529e-07,
      "loss": 0.0692,
      "step": 166
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.3324490189552307,
      "learning_rate": 2.965053449016652e-08,
      "loss": 0.1444,
      "step": 168
    }
  ],
  "logging_steps": 2,
  "max_steps": 168,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 20,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 758631591051264.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
