{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Week 5: Supervised Finetuning (SFT) - I\n",
        "**Theme:** Teaching LLMs to Follow Instructions  \n",
        "**Project:** LoRA vs. Full Finetuning on HuggingFace with Deepspeed/TRL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d32aaf6",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìò 1. What is Supervised Finetuning (SFT)?\n",
        "SFT = Pretrained model + Instruction-following data ‚Üí Task-specific model\n",
        "\n",
        "**Supervised Fine-Tuning (SFT)** is the process of further training a pre-trained language model on a labeled dataset to specialize it for specific tasks or domains.\n",
        "\n",
        "**Key points:**\n",
        "- Builds on top of a pretrained model like LLaMA, GPT, or Mistral.\n",
        "- Uses instruction-response pairs (like question-answer).\n",
        "- Enhances instruction-following ability.\n",
        "- It's a middle stage between pretraining and alignment (e.g., RLHF).\n",
        "\n",
        "**SFT Pipeline:**\n",
        "1. Pretrained Model\n",
        "2. Supervised Dataset\n",
        "3. Fine-tuned Instruction Model\n",
        "\n",
        "\n",
        "**Example:**\n",
        "| Before SFT                 | After SFT                          |\n",
        "|---------------------------|------------------------------------|\n",
        "| Random generic responses  | Follows user instructions clearly |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 2. How to Get SFT Data\n",
        "\n",
        "**4 Types of Data Sources:**\n",
        "1. **Manual Curation**: Human-created prompts and responses.\n",
        "2. **AI-Generated**: Use GPT models to self-generate instruction data.\n",
        "3. **Open Datasets**: Alpaca, OASST1, Dolly, HH-RLHF, etc.\n",
        "4. **Data Augmentation**: Rephrasing, adding context, changing perspective.\n",
        "\n",
        "**Goal**: Create high-quality, diverse, and instruction-aligned examples.\n",
        "\n",
        "* here we use the second way to generate our data using openAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e5584115",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'question': 'Can you explain the difference between synchronous and asynchronous programming?',\n",
              "  'answer': 'Synchronous programming executes tasks sequentially, meaning each task must complete before the next one begins. In contrast, asynchronous programming allows tasks to run concurrently, enabling other operations to continue while waiting for a task to complete. This is particularly useful for I/O-bound tasks, as it can improve application responsiveness and performance.'},\n",
              " {'question': 'What is your approach to debugging a complex issue in your code?',\n",
              "  'answer': 'My approach to debugging begins with reproducing the issue consistently. Next, I analyze the code and the surrounding context to identify potential causes. I utilize debugging tools and techniques such as breakpoints, logging, and unit tests to isolate the problem. Once the root cause is identified, I implement a fix and then thoroughly test to ensure the issue is resolved without introducing new bugs.'}]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def get_ai_generated_data():\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"‚ö†Ô∏è No OpenAI API key - using placeholder data\")\n",
        "        return [{\"instruction\": \"What are your technical skills?\", \"response\": \"Python, data analysis\"}]\n",
        "\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    prompt = \"Create 2 interview Q&A pairs for a software developer in JSON format. Output only JSON.\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # If model returns Markdown-style JSON block\n",
        "    if \"```json\" in content:\n",
        "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "    elif \"```\" in content:\n",
        "        content = content.split(\"```\")[1]\n",
        "\n",
        "    try:\n",
        "        data = json.loads(content)\n",
        "        if isinstance(data, dict):\n",
        "            return data.get(\"examples\", [])\n",
        "        elif isinstance(data, list):\n",
        "            return data\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Unexpected JSON format:\", type(data))\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to parse JSON:\", e)\n",
        "        print(\"Raw content:\", content)\n",
        "        return []\n",
        "\n",
        "# Example call\n",
        "get_ai_generated_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß© 3. Formatting: ChatML\n",
        "**ChatML** is a structured dialogue format used to simulate role-based conversations during SFT training.\n",
        "\n",
        "**Structure:**\n",
        "```\n",
        "<|im_start|>user\n",
        "What's the capital of France?\n",
        "<|im_end|>\n",
        "\n",
        "<|im_start|>assistant\n",
        "Paris.\n",
        "<|im_end|>\n",
        "```\n",
        "\n",
        "**Why it matters:**\n",
        "- Improves consistency\n",
        "- Helps multi-turn dialogue modeling\n",
        "- Matches formatting expectations for LLaMA and OpenAI-style models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç 4. Full Finetune vs. LoRA\n",
        "\n",
        "\n",
        "| Aspect               | Full Fine-Tuning      | LoRA (Low-Rank Adaptation) |\n",
        "|----------------------|-----------------------|-----------------------------|\n",
        "| Trainable Params     | 100%                  | ~0.5‚Äì1%                     |\n",
        "| Memory Usage         | Very High             | Low                         |\n",
        "| Flexibility          | Maximum               | Good for most tasks         |\n",
        "| Training Time        | Longer                | Faster                      |\n",
        "| Use Case             | Critical domain shift | Resource-efficient tuning   |\n",
        "\n",
        "**Recommendation**: Use LoRA for most educational and practical settings unless full retraining is justified.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "model_name = \"microsoft/DialoGPT-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"]\n",
        ")\n",
        "lora_model = get_peft_model(base_model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° 5. DeepSpeed\n",
        "\n",
        "**DeepSpeed** is a library from Microsoft that allows efficient distributed training of large models.\n",
        "\n",
        "**Modes:**\n",
        "- **ZeRO-1/2/3** for optimizer/shard parallelism.\n",
        "- **CPU Offload** to reduce GPU memory usage.\n",
        "- **Mixed Precision** for speed and efficiency.\n",
        "\n",
        "**Best for**: Scaling training to large models like 13B+, saving memory, or training on multiple GPUs.\n",
        "\n",
        "Enables memory-efficient training. Example config:\n",
        "```json\n",
        "{\n",
        "  \"zero_optimization\": {\"stage\": 2},\n",
        "  \"fp16\": {\"enabled\": true}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è 6. TRL Package (SFTTrainer)\n",
        "\n",
        "**Transformers Reinforcement Learning (TRL)** by Hugging Face includes:\n",
        "\n",
        "- `SFTTrainer`: Simplified supervised training loop.\n",
        "- `PPOTrainer`: RLHF with Proximal Policy Optimization.\n",
        "- `DPOTrainer`: Direct Preference Optimization.\n",
        "- `RewardTrainer`: For reward model training.\n",
        "\n",
        "**Why TRL?**\n",
        "- Abstracts away complex setup.\n",
        "- Faster experimentation.\n",
        "- Supports all major fine-tuning and alignment workflows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c9abd4236cf4d9b9bc7d560872dbd4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59985104c2ad45efae11a3ccae94b0ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf078f206ef640d7aa88d8c5e743344d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f308f07e72d44a99d6fea53093c2a86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "/Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>9.589100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>9.283500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=9.436296463012695, metrics={'train_runtime': 1.1459, 'train_samples_per_second': 1.745, 'train_steps_per_second': 1.745, 'total_flos': 18722451456.0, 'train_loss': 9.436296463012695})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for CUDA mechain run following \n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" \n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "\n",
        "# Prepare the dataset (must have a 'text' field)\n",
        "demo_dataset = Dataset.from_list([\n",
        "    {\"text\": \"Human: What is Python?\\nAssistant: Python is a programming language.\"},\n",
        "    {\"text\": \"Human: How do I learn coding?\\nAssistant: Start with basic concepts and practice regularly.\"}\n",
        "])\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./trl_sft_demo\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=5e-4,\n",
        "    logging_steps=1,\n",
        "    save_steps=10,\n",
        "    save_total_limit=1,\n",
        "    fp16=False,\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "# ‚úÖ Initialize SFTTrainer ‚Äî no config, no extras\n",
        "trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=demo_dataset\n",
        ")\n",
        "\n",
        "# ‚úÖ Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a99c70d",
      "metadata": {},
      "source": [
        "Explanation:\n",
        "1. global_step=2\n",
        "\n",
        "    This means the training process completed 2 optimization steps (i.e., two batches were processed and used to update the model's parameters).\n",
        "\n",
        "2. training_loss=9.432311534881592\n",
        "\n",
        "    This is the final training loss averaged over the training steps. A higher loss suggests the model hasn't learned much yet, likely because:\n",
        "        * It‚Äôs early in training (only 2 steps).\n",
        "        * The model needs more tuning or a better learning rate.\n",
        "        * The data might be complex or noisy.\n",
        "\n",
        "3. train_runtime=2.921\n",
        "\n",
        "    Total time in seconds the training took ‚Äî in this case, around 2.9 seconds.\n",
        "\n",
        "4. train_samples_per_second=0.685\n",
        "\n",
        "    The average number of training examples processed per second. Since only 2 steps were taken, the dataset or batch size may have been small.\n",
        "\n",
        "5. train_steps_per_second=0.685\n",
        "\n",
        "    How many steps (i.e., parameter updates) were completed per second. It matches the sample rate, implying 1 sample per step.\n",
        "\n",
        "6. total_flos=18722451456.0\n",
        "\n",
        "    The total number of floating point operations (FLOPs) executed during training. It's a proxy for how computationally intensive the training was.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f662325",
      "metadata": {},
      "source": [
        "### Explanation of Each Metric during model training:\n",
        "- ‚úÖ loss\n",
        "    *What it is*: Measures the model's prediction error ‚Äî how far off the model is from the target output.\n",
        "\n",
        "    *What to look for*: We want this to decrease over time.\n",
        "\n",
        "    A value of 0.1097 or 0.1366 is relatively low, which is promising, assuming it continues trending downward.\n",
        "\n",
        "    Temporary small increases (like from 0.1097 to 0.1366) can happen due to learning rate fluctuations or noisy batches.\n",
        "\n",
        "- ‚úÖ grad_norm (Gradient Norm)\n",
        "    *What it is*: L2 norm of the gradients ‚Äî essentially, how large the updates to the model's weights are.\n",
        "\n",
        "    *What to look for*:\n",
        "\n",
        "    If this value is too large, it may indicate exploding gradients.\n",
        "\n",
        "    If too small (near zero), it may mean vanishing gradients or that training is plateauing.\n",
        "\n",
        "    for example:\n",
        "\n",
        "    0.969 ‚Üí healthy magnitude, meaning the model is still learning.\n",
        "\n",
        "    0.278 ‚Üí much smaller, which could mean learning is slowing down ‚Äî possibly nearing convergence, or may need LR adjustment.\n",
        "\n",
        "- ‚úÖ learning_rate\n",
        "    *What it is*: The rate at which the model updates its weights. Often decays over time (e.g., cosine scheduler).\n",
        "\n",
        "    *What to look for*:\n",
        "\n",
        "    A decaying learning rate is common and helps fine-tune the model toward convergence.\n",
        "\n",
        "    0.000126 ‚Üí slightly higher; 0.000120 ‚Üí lower. This drop suggests a learning rate schedule is being applied, as expected.\n",
        "\n",
        "- ‚úÖ epoch\n",
        "    *What it is*: Indicates how far along you are in training (e.g., 4.67 = 67% through the 5th epoch).\n",
        "\n",
        "    *What to look for*: Helps track progression. You‚Äôd want to compare loss and grad_norm across epochs to evaluate learning trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb59b478",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4094f035",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "\n",
        "You‚Äôve learned:\n",
        "- What SFT is and why it‚Äôs essential\n",
        "- Where and how to get quality data\n",
        "- How to use ChatML format\n",
        "- When to choose LoRA vs full tuning\n",
        "- How to leverage DeepSpeed and TRL for scale and alignment\n",
        "\n",
        "## - For the full llama3 sft code, check out class_5_llama3.py"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
